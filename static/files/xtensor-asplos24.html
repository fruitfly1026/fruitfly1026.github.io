---
layout: workshop
title: XTensor2024
---

<!DOCTYPE html>

<center><h1>XTensor 2024</h1></center>
<center><h2>1st Workshop on Cross-stack Optimization of Tensor Methods</h2></center>
<center><p>April 27 1-5pm, 2024 @ San Diego, CA, USA</p></center>
<center><p>Held in conjunction with <a href="https://www.asplos-conference.org/asplos2024/">ASPLOS'24</a></p></center>

</br>

<p>Tensor problems are becoming ever more important to represent data and analyze its inherent properties, as the correlation of data gains importance in many domains. The application of tensor methods covers machine learning/deep learning, quantum chemistry/physics, quantum circuit simulation, social networks, and healthcare, to name a few. The research on tensor methods comes from multiple domains, including computer architecture, programming languages, compilers, and parallel computing.  This workshop aims to gather researchers from diverse computer system backgrounds to present and communicate their work on tensor methods, and then seek a cross-stack solution to improve the performance of tensor algorithms.</p>

<h2>Call for Papers</h2>

<p>XTensor is a venue for discussion and brainstorming at the intersection of software and hardware research. </p>
<p>Research topics includes, but not limited to:</p>
<ul>
	<li>Research angles:</li>
	<ul>
		<li>Programming abstractions</li>
		<li>Compiler techniques</li>
		<li>Runtime optimization</li>
		<li>Libraries/frameworks</li>
		<li>High-performance algorithms</li>
		<li>Hardware architecture</li>
		<li>Performance model</li>
	</ul>
	<li>Tensor methods:</li>
	<ul>
		<li>Data: dense, sparse, structured, symmetric layout; random or non-negative values; etc.</li>
		<li>Randomized, approximate, etc.</li>
		<li>Tensor operators, such as tensor products, tensor-matrix multiplication, tensor-tensor multiplication, matrix-matrix multiplication</li>
		<li>Tensor decompositions, such as Canonical polyadic decomposition (CPD), Tucker decomposition, etc.</li>
		<li>Tensor networks, such as tensor train, tensor ring, hierarchical Tucker, the projected entangled pair states (PEPS), etc.</li>
		<li>Tensor regression, tensor component analysis, tensor-structured dictionary learning, etc.</li>
	</ul>
	<li>Platforms:</li>
	<ul>
		<li>CPUs</li>
		<li>GPUs (e.g., NVIDIA, AMD, Intel)</li>
		<li>AI accelerators (e.g., SambaNova, Graphcore, Habana, GroqRack)</li>
		<li>Wafer-scale system (e.g., Cerebras)</li>
		<li>FPGAs</li>
		<li>ASICs</li>
		<li>Any kinds of simulators</li>
	</ul>
</ul>


<h2>Position Papers Submission</h2>
<p>Discussion and communication are the primary goals of the workshop. Thus, we only ask for <strong>2-page position papers</strong>. The submitted work is very flexible in content. It could be early, in-progress research; could have not very complete experiments or results; could be a valuable survey of recent research trends or tools; could propose an important open question and call for solution; could be an experience report even with failed approaches but with some lessons learned. Submitted papers will undergo peer review by a program committee of experts from diverse research domains working on tensor problems.</p>
<p>Papers should follow the two-column formatting guidelines for <a href="https://www.acm.org/publications/proceedings-template">SIGPLAN template</a> (the acmart format with the sigplan two-column option) and <strong>up to 2 pages</strong>, excluding references. Review is single-blind, please include authors' names on the submitted PDF. </p>
<p>Paper submission will be via <a href="https://easychair.org/conferences/?conf=xtensor24">EasyChair</a>. The accepted papers will not be published in a proceeding. Presentation slides will show on the workshop website.</p>


<h2>Important Dates</h2>
<ul>
<li>Paper submission: <strong>March 25, 2024</strong></li>
<li>Author Notification: <strong>April 8, 2024</strong></li>
<li>Workshop: <strong>April 27 1-5pm, 2024</strong></li>
</ul>
All deadlines are AOE 11:59pm.


<h2>Organization</h2>

<h3>Chairs:</h3>
<a href="https://arcb.csc.ncsu.edu/~mueller/">Frank Mueller</a>, North Carolina State University<br/>
<a href="https://faculty.ucmerced.edu/dong-li/">Dong Li</a>, University of California, Merced<br/>
<a href="https://web.engr.oregonstate.edu/~chenliz/">Lizhong Chen</a>, Oregon State University<br/>
<a href="http://jiajiali.org">Jiajia Li</a>, North Carolina State University, <a href="mailto:jiajia.li@ncsu.edu">jiajia.li@ncsu.edu</a><br/>

<h3>Committee:</h3>

<a href="http://jeewhanchoi.com/">Jee Choi</a>, University of Oregon<br/>
<a href="https://bastianhagedorn.github.io">Bastian Hagedorn</a>, NVIDIA<br/>
<a href="https://ammhelal.github.io/">Ahmed Helal</a>, Intel<br/>
<a href="https://www.cs.wm.edu/~bren/">Bin Ren</a>, William & Mary<br/>
<a href="https://www.jokeren.tech/">Keren Zhou</a>, George Mason University<br/>


</br><p><strong>Please contact Jiajia Li at <a href="mailto:jiajia.li@ncsu.edu">jiajia.li@ncsu.edu</a> if you have any questions.</strong></p></br>